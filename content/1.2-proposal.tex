\subsection{Research questions}
% This work compares the use of Multilayer Perceptrons (MLP), Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), and combinations of those types of networks, to perform file fragment classification, which is the first step of the data carving process, identification, answering the following initial questions:
This work explores some of the challenges of applying neural networks in file fragment classification, attempting to answer the following research questions:

%from pep 4.1
\begin{enumerate}[itemindent=\parindent,label=\textbf{Q\arabic*.}]

% In the first, the accuracy of some types of neural networks is compared, classifying file fragments taken from the Govdocs1 dataset\cite{garfinkel_bringing_2009}, using their file extensions as class labels. 
    \item How do different neural network models compare to each other in terms of training performance and quality of results?
    
% Then, the influence of the number of classes on the accuracy of the resulting models is briefly explored.
    \item How does the accuracy of neural network models change relative to the number of classes?

% Finally, an experiment is devised to measure, for a given neural network architecture, the number of fragments that have recognizable structures for each file type.
    \item Can high entropy data misinterpreted as random data explain part of the file fragment classification errors? If so, to what extent?
\end{enumerate}

The initial goal of the the first research question was to identify the most promising models on file fragment classification, but an apparent limit was found on how far these models could be improved. This led to the second research question, that explores the influence of number of classes on the accuracy of the models. While the number of classes seems to influence the accuracy of the trained models, the choice of which file types are included in the dataset have a bigger impact because file types that contain images or use compression were found to have a expressive negative effect on accuracy. Motivated by these results, the third question was formulated to test the hypothesis that part of the observed errors are caused by high entropy data misinterpreted as random data.


In this work, only neural networks are taken into account, but other machine learning approaches could also be applied, like Support Vector Machines (SVM) and k-Nearest Neighbors (kNN). This restriction was motivated by the success that neural networks have shown in other fields, like image classification and speech recognition.
% The flexibility of neural networks to combine different types of layers is also important, as it is a core characteristic being explored in this research.

% Two sets of experiments were conducted. In the first set, the initial 512 bytes of a file is used as input to the tested neural network, whose task is to predict the file type. The second set is similar, but the 512 bytes fragment is extracted from a random position of the file, which is a more difficult task as it cannot depend on header patterns.
