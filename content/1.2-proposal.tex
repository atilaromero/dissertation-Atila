\subsection{Research questions}
% This work compares the use of Multilayer Perceptrons (MLP), Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), and combinations of those types of networks, to perform file fragment classification, which is the first step of the data carving process, identification, answering the following initial questions:
This work explores some of the challenges of applying neural networks in file fragment classification, attempting to answer the following research questions:

%from pep 4.1
\begin{enumerate}[itemindent=\parindent,label=\textbf{Q\arabic*.}]

% In the first, the accuracy of some types of neural networks is compared, classifying file fragments taken from the Govdocs1 dataset\cite{garfinkel_bringing_2009}, using their file extensions as class labels. 
    \item How do different neural network models compare to each other in terms of training performance and quality of results?
    
% Then, the influence of the number of classes on the accuracy of the resulting models is briefly explored.
    \item How does the accuracy of neural network models change relative to the number of classes?

% Finally, an experiment is devised to measure, for a given neural network architecture, the number of fragments that have recognizable structures for each file type.
    \item Can high entropy data misinterpreted as random data explain part of the file fragment classification errors? If so, to what extent?
\end{enumerate}

The initial goal of the the first research question was to identify the most promising models on file fragment classification, but an apparent limit was found on how far these models could be improved. This led to the second research question, that explores the influence of number of classes on the accuracy of the models. While the number of classes seems to influence the accuracy of the trained models, the choice of which file types are included in the dataset have a bigger impact because file types that contain images or use compression were found to have a expressive negative effect on accuracy. Motivated by these results, the third question was formulated to test the hypothesis that part of the observed errors are caused by high entropy data misinterpreted as random data.

\todo{compare with comments}

% Section \ref{sec:evalmodels} evaluates some alternative models in the file fragment classification task. The expectation was to identify the most promising models for improvement. Instead, an apparent limit was found on how far these models could be improved. 

% Section \ref{sec:numberofclasses} describes how does the accuracy of neural network models change relative to the number of classes. It was observed that high accuracy in the file fragment classification task could be achieved only when the number of classes was small. Also, when compared with each other, file types with high entropy data showed the lowest accuracy values.

% Section \ref{sec:exprandom} investigates the hypothesis that some part of these errors may be explained by the inability of the models to distinguish high entropy data from random data. The portion of data that could be identified as not random was higher than expected. While some of the errors could have the explanation mentioned in the hypothesis, it could only explain about 1/3 of the observed errors (16.5\% out of 46\%) \todo{revise numbers}.

In this work, only neural networks are taken into account, but other machine learning approaches could also be applied, like Support Vector Machines (SVM) and k-Nearest Neighbors (kNN). This restriction was motivated by the success that neural networks have shown in other fields, like image classification and speech recognition.
