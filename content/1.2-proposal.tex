\subsection{Research questions}
% This work compares the use of Multilayer Perceptrons (MLP), Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), and combinations of those types of networks, to perform file fragment classification, which is the first step of the data carving process, identification, answering the following initial questions:
This research explores to what extent the presence of high entropy data can explain classifications errors in the file fragment classification task by answering the following initial questions:

%from pep 4.1
\begin{enumerate}[itemindent=\parindent,label=\textbf{Q\arabic*.}]

% In the first, the accuracy of some types of neural networks is compared, classifying file fragments taken from the Govdocs1 dataset\cite{garfinkel_bringing_2009}, using their file extensions as class labels. 
    \item How do different neural network models compare to each other in terms of training performance and quality of results?
    
% Then, the influence of the number of classes on the accuracy of the resulting models is briefly explored.
    \item How does the accuracy of neural network models change relative to the number of classes?

% Finally, an experiment is devised to measure, for a given neural network architecture, the number of fragments that have recognizable structures for each file type.
    \item Can high entropy data misinterpreted as random data explain part of the file fragment classification errors? If so, to what extent?

\end{enumerate}

In this work, only neural networks are implemented, but other machine learning approaches exist, like Support Vector Machines (SVM) and k-Nearest Neighbors (kNN). This choice of restriction was motivated by the success that deep neural networks have shown in other fields, like image classification and speech recognition.
% The flexibility of neural networks to combine different types of layers is also important, as it is a core characteristic being explored in this research.

% Two sets of experiments were conducted. In the first set, the initial 512 bytes of a file is used as input to the tested neural network, whose task is to predict the file type. The second set is similar, but the 512 bytes fragment is extracted from a random position of the file, which is a more difficult task as it cannot depend on header patterns.
