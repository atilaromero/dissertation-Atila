% \todo[inline]{include method figure}

% \levelB{Model}
The model architecture used in this research stage, ``CLD'', is described in the previous chapter and has three layers. The first is a convolutional layer with 256 output units, a window of size 16, and a stride of 16. The second is an LSTM layer of 128 units. The last is a fully-connected layer where the number of output units matches the number of classes of the input dataset.
The same dataset handling, sampling, output, and input encoding described in Chapter 4 were used here.

%optimization
All the training sessions used the Adam \cite{kingma_adam:_2014}
optimization algorithm to guide backpropagation, which was selected because it performed well in the preliminary tests without requiring fine-tuning of parameters.

% Constraints
The models were trained until no further improvement was observed in the last 10 epochs, using categorical cross-entropy loss.

\levelC{Accuracy vs. number of classes}
Several independent models with the same general architecture, but different weights, were trained with 2 to 28 extensions from the Govdocs1 dataset. For each number in the range 2 to 28, a random subset of extensions was selected to compose the dataset. Each extension has 200 file samples, half placed in the training dataset and half in the validation dataset. Each dataset was used to train a new model, that should distinguish between the selected subsets of extensions in that filtered dataset. This process was repeated five times.

\levelC{Accuracy of pairs of classes}

In addition to this sampling of extensions using different quantities of classes, all 378 combinations of pairs of extensions were compared. Each pair was used to compose a dataset and to train a new model for each one, using the same process described in the previous section.

Given the number of different models trained (135 for the N classes phase and 378 for the pairs), it was important to use a model architecture that had a fast training time. Models trained with the chosen architecture took about 10 minutes to train. Without this short training period, this research part would not be feasible.

\levelC{Principal Component Analysis}
The accuracy obtained after the training of models with pairs of classes was used to build a 28x28 matrix. Here the accuracy of the model trained for that particular pair of extensions was used as a distance measure of how similar those file types are. If a model was able to achieve 100\% accuracy, the file types would be considered different, if the accuracy is 50\%, they would be considered indistinguishable, as 50\% is the average accuracy of a random guess classifier for two classes.

Following this concept, the diagonal entries of the matrix were filled with the value 0.5, as each class should be indistinguishable from itself. Then, a Principal Component Analysis (PCA) \cite{amirani_new_2008} was used to treat this matrix. The 28x28 matrix can be interpreted as 28 vectors of 28 dimensions. For each class, the vector indicates how similar that class is to each of the others. This could be plotted in a space of 28 dimensions if such representation could be built. What PCA does is to reduce the dimensionality of this space. Here the dimensions were reduced to two, to be shown in a 2D graph. To do that, it chooses as first axis the one that maximizes the variance in the data and then does the same for the second dimension. That produces a projection in which the points are as farther away as possible.

While it may not be obvious how to attribute meaning to the resulting coordinate system, this technique is useful to visually group classes with similar characteristics.

