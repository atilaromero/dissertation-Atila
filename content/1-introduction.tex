\chapter{\label{chap:introduction}Introduction}

%from pep 1, paragrafo 1
In a forensic context, file recovery is a frequent task that can be motivated by several situations, like physical media malfunction, intentional attempt to hide data, and the need to access deleted or older versions of files. When the filesystem no longer provides the physical location of a file on the media, data carving is often the only procedure capable of retrieving its content.

\subsection{Motivation}

%from pep 1, paragrafo 4
The patterns searched by data carving software are generally manually coded, taking advantage of fixed byte sequences found on headers and footers. But the amount of different file types combined with the slow process of manually coding each of those patterns makes the development of data carving software a tedious task \cite{mcdaniel_content_2003}.

%from pep 1, paragrafo 5
The application of machine learning solutions to this manual task has the potential to make it easier and faster. An initial strategy could be to train a classifier to, given a chunk of data, provide a label indicating a file type. That could be used to recover unfragmented deleted files.

%from pep 1, paragrafo 6
The recovery of fragmented files through data carving would require some sort of pattern recognition on the identified chunks, in order to reconstruct the correct sequence.

\subsection{challenges}

%from pep 4, paragrafo 4
The work of Hiester \cite{hiester_file_2018} has shown that LSTM neural networks have good potential in the data carving field. But, as happens with studies applying LSTM models to speech recognition, where many researchers have contributed with different models, adjustments, modifications, and innovations, also in the field of data carving field it is important to further advance the research. Important aspects that require attention include support for a wider range of file types, handle fragmentation through reassembling, and make the task of supporting the carving of a new file type easier.

%from pep 4, paragrafo 5
Each of the steps cited by Ali et al. \cite{ali_review_2018} for the data carving process deals with a main challenge. The identification step is responsible for classifying the file type. The quantity and diversity of file types, together with the accuracy and precision of the results, are the main challenges in identification. The second step, validation, also deals with classification, but it is a complementary step to the previous one to reduce false positives, often using a different technique. For that reason, validation challenges are similar to the identification ones. The last step, reassembling, has fragmentation as its main challenge.

%from pep 4, paragrafo 6
In the current proposal, a new type of challenge is introduced. Instead of using previous knowledge of the file structure to improve carving results, \textbf{would be possible to do the inverse and use insights from the carving process to reveal structures in the file?}

%from pep 4, paragrafo 7
% For example, suppose a file has a fixed size field, a 32 bit unsigned integer representing a datetime value. 
% For that type of field, the insight may come in the form of an expected range. Still using the datetime example, a possible outcome would be the observation that a certain kind of file always presents that field value inside some range, that coincides with a range often observed in datetime fields. That does not prove the unknown field to be a datetime but suggests that direction.

%from pep 4, paragrafo 8
% Few structures are so simple as a group of fixed sized fields. It is very common, for example, to use a field to specify the length of the next field. Another type of complexity increase occurs when the value of a field establishes which specification should be used in the remaining of the file, changing which fields should be expected next.

% %from pep 4, paragrafo 9
% The greater the complexity of an unknown file structure is, the more difficult it is to unravel its specification, but also the more useful it is to count with tools that automatize that task. Otherwise, the only option is to manually write the specification or the parser, possibly relying on reverse engineering techniques.

%from pep 4, paragrafo 10
% The direct utility of the discovery of file type structures is the extraction of values from its fields. This information has value for itself, but could also be used to improve validation and even reassembling.

%from pep 4, paragrafo 11
% Another concern that can be explored and is pertinent to possibly all of the previous studies combining neural networks and data carving is that their datasets and results may not reflect the same situations that would be faced on a real forensic scenario, where the distribution of file types may be different and the occurrence of unknown and untreated formats may be frequent. Therefore, more realistic datasets are important to improve research validation. 


\subsection{Research questions}
\subsection{Hypothesis}

\subsection{Objectives}
\todo[inline]{review objectives}
%from pep 1, paragrafo 7 e pep abstract, paragrafo1
This work explores the use of Long Short-Term Memory (LSTM) neural networks to perform data carving, investigates how the construction of data carving software can be fully or partially automated using this technology, and apply the findings towards a practical solution in which the forensic examiners community can collaboratively build models for several file types.

\section{Objectives}

%from pep 4.1, paragrafo 1
The main objective of the proposed work is to advance the research on the use of Long Short-Term Memory (LSTM) neural networks to perform data carving, answering the following initial questions:

%from pep 4.1
\begin{enumerate}[itemindent=\parindent,label=\textbf{Q\arabic*.}]
  \item How does LSTM compare to other techniques in terms of training performance and quality of results?
  \item Do the results obtained with usual datasets reflect what happens in real scenarios?
  \item Could a LSTM-based tool support a wider range of file types?
   \item Could a LSTM-based tool handle fragmentation through reassembling?
   \item Do LSTM neural networks help to interpret internal file structures?
  \item How can the process of adding support for a new file type be made easier?
  \item Could a LSTM-based tool accept the addition of new file types by the end-user, in a collaborative manner? 
  \end{enumerate}



\section{Metrics and datasets}

%from pep 4.2
In order to compare solutions, a methodology of comparison must be defined in the early stages of the research, specifying datasets and metrics.

%from pep 4.2
For datasets, some of the most used ones were the Digital Forensic Research Workshop (DFRWS) 2006-2007 dataset \cite{qiu_new_2014}, \cite{ali_review_2018}, and GovDocs \cite{hiester_file_2018}, \cite{fitzgerald_using_2012}, \cite{beebe_sceadan:_2013}.

%from pep 4.2
Classic data carving tools, namely Scalpel \cite{richard_iii_scalpel:_2005}, Foremost \cite{kendall_foremost_2019} and Photorec \cite{grenier_photorec_2019}, can be used to process those datasets, to create a research baseline.

%from pep 4.2
For metrics, usual choices like accuracy, precision, recall, and f1-score can be used to measure the quality of the results. Model training time is also an important metric to consider.

%from pep 4.2
To measure the ease with which the end user could add new file types, a possible solution is to use a survey, but that can only be made in the later stages of the work after a minimal working tool is devised.


\section{Testing environment}
%from pep 4.3
Before starting the tests, an appropriate environment must be built. This requires an evaluation of available machine learning tools and frameworks. Prominent options include Keras \cite{chollet_keras_2019} and TensorFlow \cite{google_brain_tensorflow_2019}.

%from pep 4.3
During evaluation of those tools, while building the environment, some of the most simple solution alternatives can begin to be tested, probably training binary classification of file types, using a simple dataset consisting of small files. This will allow to both compare frameworks and to begin the tests.

\section{Experiments}
%from pep 4.4
Following the environment preparation, the next step should be to compare solutions, starting with the most simple solutions first, and increasing complexity next. Planned neural networks to test include feedforward, convolutional, LSTM and BLSTM. For comparison of results, SVM and kNN can also be explored.

%from pep 4.4
Some of the possible experiments that can be conducted are:
%
manipulation of datasets to measure results on different scenarios,  shuffling data to simulate fragmentation, for example, or by removing portions of files to simulate data corruption;
%
increase in the number of supported file types, investigating the best strategy to scale the solution;
%
try to reassemble fragmented files using different neural network architectures;
%
research ways to share trained models, an essential requirement to give practical applicability to the research;
%. Some alternatives should be probed and some solutions outlined.
%
adaption of visualization techniques of neural networks, attempting to infer file structure.


\subsection{Outline}

The remainder of this document is organized as follows.
\todo[inline]{check document outline}
    Chapter 2 analyses the current status of data carving tools and research. 
    Chapter 3 analyses how current research on sequence labeling can improve data carving solutions.
    Chapter 4 proposes solutions to some of the data carving problems.
    Chapter 5, for each performed experiment, describes the chosen method, presents the results obtained and offers an discussion of the results.
    Chapter 6 summarizes the work, analysing achievements and limitations, and including suggestions for future work.
