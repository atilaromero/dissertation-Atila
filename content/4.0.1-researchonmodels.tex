\levelB{Evaluation of models based on accuracy}
\levelC{Objective}
In this experiment, different neural networks are trained and validated
at the filetype identification task. Their accuracy is then compared,
to identify which models should be considered or disregarded in the remaining phases.

\levelC{Dataset}
The Govdocs1 dataset \todo{citation} was fully downloaded and its files were grouped by extension. This dataset has files with 63 different extensions. The 33 extensions with less than 200 files were discarded. From the remaining 30 extensions, listed in table \ref{tab:govdocs1}, 200 files of each were randomly selected, 100 to use in the training dataset and 100 to use in the validation dataset.

\input{content/tables/4.0.1-govdocs.tex}

\levelC{Sampling}
To select a sample instance from the dataset, first a random file is selected among those available, without reposition. Then, a random block from this file is randomly selected. After all files have participated in the process, the process may be repeated as long as necessary. This way, all files are considered and the classes are easily balanced.
This contrasts with the sampling technique applied in other works, where all the sectors are grouped together before sampling, which may lead to a imbalance between classes.

\levelC{Inputs}
The input features of the network for each instance is a 512x8 matrix representing only one block of 512 bytes of a random file of the dataset. Each of the 512 bytes uses a custom encoding where each of the 8 bits of a byte is represented as -1 or 1, depending whether the bit is 0 or 1. During initial tests, this encoding was compared to three other encodings: one-hot encoding, 8 bits represented as 0 or 1\todo{include citation}, and 8 bits represented as [0,1] and [1,0] \cite{hiester_file_2018}. More research should be done in the future to determine the best of the four, but initial results suggest they have similar impact on the model accuracy. The one-hot encoding has the disadvantage of increasing the input matrix size by a factor of 32.

\levelC{Outputs}
The output of the network for a given instance is a vector with a size equal to the number of classes, subjected to a softmax function, which applies the exponential function on the vector and then normalizes it. Each value will represent the predicted probability that the instance belongs to a specific class.


\levelC{Models}
The networks considered used different combinations of convolutional, max pooling, LSTM, and fully connected layers,
%loss
using categorical crossentropy as the loss function. Binary crossentropy and mean squared error were considered during initial tests, but categorical crossentropy has showed faster training times.

%optimization
All the experiments use the Adam \cite{kingma_adam:_2014}
optimization algorithm to guide backpropagation, which was selected because it performed well in the preliminary results without fine tuning of parameters.

% Constraints
The models where trained with a time constraint of ten minutes, which was sufficient for most of the models to show stagnation in learning.

\levelC{Hardware}
The experiments did not take advantage of GPU acceleration and were  conducted on a single computer with 256GB of RAM and with 2 Intel\textregistered Xeon\textregistered E5-2630 v2 processors, with 6 cores each, with 2 hyper-threads per core, or 24 hyper-threads in total. 


\levelC{Software}
\tudo[inline]{python, keras, tensorflow, linux, jupyter notebook}

% repository
The source code for the experiments is available at \url{http://github.com/atilaromero/ML}.
\todo[inline]{create a cleaner repository just for the paper}

\levelC{Results}

%stagnation in learning with such short times, combined with low final accuracy suggests that the dataset present some patterns that are easily recognizable, while the rest of the dataset present a very hard classification task.

\levelC{Limitations and threats to validity}







Dissemination:
-- dataset
-- authorship
-- repository
-- id and citation
-- data management plan
-- diary
-- experimental issues
