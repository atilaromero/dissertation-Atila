\section{Research methods}
% providing a synopsis of the research method(s)


Due to the lack of reproducible research applying LSTM to perform data carving, the research method adopted in this study starts with the validation of the development environment. 

This step aims to ensure that both the tools and the techniques used the rest of the study are properly employed. An error on an algorithm implementation could disrupt the neural network training in various degrees, which could lead to the incorrect discard of an otherwise valid solution or to an excessive training time, invalidating further comparison of the resulting models. A validation procedure applied on the development environment mitigates those possible risks. 

To perform this validation, three set of tasks were used. The first was to reproduce results of an Recurrent Neural Network (RNN) exercise from Coursera \todo{reference}, originally in python, using Keras. The second was to recreate the model of a Keras' LSTM exercise, also from the same Coursera course. The third was to train a model for a speech recognition problem, without a reference model to rely on, but knowing that the task is possible. 

Each of these three tasks is more difficult than the previous one and check for higher levels of abstraction. First the base framework is checked for exact results, then the procedure of creation of a LSTM model is checked, and finally the procedure of exploration of different models is checked.

Each task can rely on some external reference in the case of eventual failure. For the first two, the implementation reference is the obvious fallback. For the third one, if the attempts of training a neural network for speech recognition were unsuccessful, some of the successful results in the literature could be used to identify the problem. In contrast, should the data carving experiment fail, there would be no other implementation with available source code to compare solutions to.

% TensorFlow \cite{abadi_tensorflow:_2016} and Keras \cite{chollet_keras_2019} are the basic tools used in the development environment of this work. 

The research data used in the speech recognition task was generated using Espeak \todo{reference}, an open source text-to-speech software. 

Following the validation of the development environment, the next step is to use this environment to conduct the data carving experiments.

Andrew Ng\todo{reference} recommends the practice of having three types of datasets: ``training'', ``development'' and ``test''. Only the training dataset should be used to train the neural network. The development dataset is used to evaluate the chosen metrics, and may be splitted in ``eyeball'' and ``blackbox''. The optional eyeball dataset can have its errors manually inspected, while the blackbox dataset should be uninspected, to avoid overfitting. The test dataset is used to check if the development dataset have overfitted.

Both the development and test dataset must come from the same distribution, and is preferable that this distribution represents the data where the neural network is supposed to perform well. An often disregarded characteristic, specially relevant to small datasets, is that the train dataset does not have the same requirement.

Using this assumption, two datasets frequently mentioned in the literature when comparing data carving solutions were selected to be used as development and test datasets, the dfrws-2007-challenge \todo{reference} and the Govdocs1 \todo{reference}, while the training dataset would be constructed from other sources.



starting with a identification task of a few file types, to try to achieve results similar to those described by Hiester \cite{hiester_file_2018}.

At this stage, accuracy is the main target metric, with training time as a secondary metric and constraint.


The datasets 


\todo[inline]{git/folders procedure}



\section{text from pep}
%Metrics and datasets

% %from pep 4.2
% In order to compare solutions, a methodology of comparison must be defined in the early stages of the research, specifying datasets and metrics.

% %from pep 4.2
% For datasets, some of the most used ones were the Digital Forensic Research Workshop (DFRWS) 2006-2007 dataset \cite{qiu_new_2014}, \cite{ali_review_2018}, and GovDocs \cite{hiester_file_2018}, \cite{fitzgerald_using_2012}, \cite{beebe_sceadan:_2013}.

% %from pep 4.2
% Classic data carving tools, namely Scalpel \cite{richard_iii_scalpel:_2005}, Foremost \cite{kendall_foremost_2019} and Photorec \cite{grenier_photorec_2019}, can be used to process those datasets, to create a research baseline.

% %from pep 4.2
% For metrics, usual choices like accuracy, precision, recall, and f1-score can be used to measure the quality of the results. Model training time is also an important metric to consider.

% %from pep 4.2
% To measure the ease with which the end user could add new file types, a possible solution is to use a survey, but that can only be made in the later stages of the work after a minimal working tool is devised.

% Test environment

% %from pep 4.3
% Before starting the tests, an appropriate environment must be built. This requires an evaluation of available machine learning tools and frameworks. Prominent options include Keras \cite{chollet_keras_2019} and TensorFlow \cite{google_brain_tensorflow_2019}.

% %from pep 4.3
% During evaluation of those tools, while building the environment, some of the most simple solution alternatives can begin to be tested, probably training binary classification of file types, using a simple dataset consisting of small files. This will allow to both compare frameworks and to begin the tests.

%from pep 4.4
Following the environment preparation, the next step should be to compare solutions, starting with the most simple solutions first, and increasing complexity next. Planned neural networks to test include feedforward, convolutional, LSTM and BLSTM. For comparison of results, SVM and kNN can also be explored.

%from pep 4.4
Some of the possible experiments that can be conducted are:
%
manipulation of datasets to measure results on different scenarios,  shuffling data to simulate fragmentation, for example, or by removing portions of files to simulate data corruption;
%
increase in the number of supported file types, investigating the best strategy to scale the solution;
%
try to reassemble fragmented files using different neural network architectures;
%
research ways to share trained models, an essential requirement to give practical applicability to the research;
%. Some alternatives should be probed and some solutions outlined.
%
adaption of visualization techniques of neural networks, attempting to infer file structure.
