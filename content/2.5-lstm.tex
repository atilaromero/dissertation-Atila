Recurrent Neural Network (RNN) are neural networks where part of the input of a layer uses its previous output.
Long Short-Term Memory \cite{hochreiter_long_1997} (LSTM) is a type of Recurrent Neural Network architecture that addresses the problem of vanishing or exploding gradients through time. This problem occurs in regular RNNs because the signal transmitted from one time-step to another is influenced at each interaction, both on forward and backward propagation phases, limiting the potential interaction between distant time-steps. The solution offered by LSTM introduces the concept of a memory cell, adding gates units to each node. The initial proposal included only input and output gates \cite{hochreiter_long_1997}.
The input gate is responsible for regulating if the cell internal state will be affected by the layer input. Similarly, the output gate regulates if the internal state will influence the output of the layer. The concept of a forget gate was introduced later \cite{gers_learning_2000}, to control whether the internal state of the cell should be reset.

Equations \ref{eq:lstmequationsstart} to \ref{eq:lstmequationsend} describe a typical LSTM network. At each time step $t$, $x_t$ is the input vector, $c_t$ is the cell's internal state and $h_t$ is the output state. Both $c_t$ and $h_t$ influence the next time step as variables $c_{t-1}$ and $h_{t-1}$. The symbol $\circ$ denotes an element wise multiplication.

The result of the forget, input, and output gates is represented by $f_t$, $i_t$, and $o_t$.
In each of the three gates, $W$ is the set of weights to be applied to the input $x$, while $U$ is the set of weights to be applied on the previous time step output, $h_{t-1}$. 
The dimensions of $x$ and $h_{t-1}$ are respectively the chosen dimensions for input and output, while the dimension for $W$ and $U$ are such that the output has the same dimensions of $h_{t-1}$.  Symbol $b$ denotes the bias vector, having the same dimensions of both $h$ and $h_{t-1}$. Symbol $\sigma_g$ denotes the activation function of the gate. Each gate is mathematically equivalent to a fully connected layer where the inputs are the concatenation of the $x$ and $h_{t-1}$ vectors and the output has the same dimensions of $h$.

The activation function usually chosen for the gates is the sigmoid, which gives values in the $[0,1]$ range. If the forget gate $f_t$ results zero for a given component, the corresponding component in the $c_{t-1}$ vector will have no influence on $c_t$ or $h_t$. If all the resulting components of the forget gate are zero, which corresponds to the zero vector, then the previous cell's state $c_{t-1}$ is completely ignored.
Similarly, if the input gate $i_t$ results in a zero component, the corresponding component in the $c_t$ vector will not be influenced by $x$ and $h_{t-1}$.

If all components of $f_t$ and $i_t$ are equal to $1$, then $c_t = c_{t-1} + \sigma_c(x_t W_{c} + h_{t-1} U_{c} + b_c)$, which is the case where $c_t$ is fully influenced by $c_{t-1}$, $x$ and $h_{t-1}$.

The output gate $o_t$ follows a similar pattern, but it regulates how $h_t$ will be influenced by $c_t$.

\begin{align}
\label{eq:lstmequationsstart}     
f_t &= \sigma_g(x_t W_{f} + h_{t-1} U_{f} + b_f) \\
i_t &= \sigma_g(x_t W_{i} + h_{t-1} U_{i} + b_i) \\
o_t &= \sigma_g(x_t W_{o} + h_{t-1} U_{o} + b_o) \\
c_t &= f_t \circ c_{t-1} + i_t \circ \sigma_c(x_t W_{c} + h_{t-1} U_{c} + b_c) \\
\label{eq:lstmequationsend}
h_t &= o_t \circ \sigma_h(c_t)
    \end{align}

% \todo[inline]{conclusion of nn section, conclusion of data carving section, and conclusion of chapter. What could possibly be concluded?!?! In this chapter we will describe y. Y=x. In this chapter we described y.}