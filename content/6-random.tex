% \levelC{Objective}
In the previous chapter, it was observed that file types that use compression or contain images have a higher negative impact on accuracy than other file types do. This raises the question of whether high entropy may be related to the observed errors.

Before conducting experiments related to the cause of errors, a list of conceivable error sources (E1 to E5) was elaborated:
\begin{enumerate}[itemindent=\parindent,label=\textbf{E\arabic*.}]
    \item For some data structure, the model cannot distinguish it from random data. This can happen if the pattern in the data is too complex, beyond the capabilities of the model. It may be the case that in practice no model can perform this distinction or, instead, this may be a limitation of this particular model only. This situation may occur in files that use compression or cryptography, or more generally, any file type with high entropy.
    
    \item Different file types using different data structures, but the model cannot distinguish between them. In this case, the model can perceive that there are structures in the data, but it is not able to differentiate them.

    \item Different file types using the same data structure. It is common for a given file type to employ different types of data structures. If two or more file types make use of the same data structure, the existence of that structure will then not be sufficient to differentiate those file types, as it may belong to any of them.
    The reverse, a file type that uses multiple kinds of data structures, does not constitute a problem as it simply results in extra work during the model training.

    \item Same file type with multiple extensions. If the same file type appears in the dataset with multiple extensions, ``JPG'' and ``JPEG'' for example, the model will not be able to predict which label is used in the validation dataset for a given instance, since this distinction exists in the labeling, but not in the content.

    \item Files that contain other files. Some file types need to embed other files. If the inner file is categorized, even if the model finds an unmistakable pattern, it will not match the label, which will be the extension of the outer file. A ``PDF'' file, for example, can embed a ``JPG'' file. If the model predicts ``JPG'' as the class of a portion of this file, it will not match the instance label on the dataset, which would be ``PDF''.
\end{enumerate}

The first error source, E1, is the focus of this chapter. In the file fragment classification task, it is expected that some portion of the classification errors of a given model be caused by its inability to recognize patterns in the data if the data have high entropy. In these situations, even humans may be unable to tell if a sample is valid or if it is random data. A search for an alternative model may lead to better results, but the chances are low, as this type of error is hard to mitigate. 

In the second error source, E2, it is conceivable that avoiding misinterpretation of one structure as another is possible since the model has successfully detected structures. This error source is not explored in this study. Mitigation of this type of error probably would be achieved through more training or by a search for a better model.

The three last error sources listed above, E3 to E5, are not explored in this study. They are similar in nature, as the last two may be viewed as special cases of E3. This type of error is best mitigated through a revision of the labeling system.

% \levelC{Hypothesis}
The goal of the experiment of this chapter is to test the implicit hypothesis of the third research question \textbf{that part of the file fragment classification errors can be explained by the inability of the models to distinguish high entropy data from random data}. Not all file types have this kind of data, and for those that do, they should compose only a portion of the file.

% Two methods are described next. The measuring method results in a number that can be used as a measure of entropy. The filtering method can be used to filter the original dataset, leaving only samples that have recognizable structures.
