\section{\label{sec:lstm}Long Short-Term Memory}
% Hochreiter schmidhuber, 1997
% Hochreiter, S., & Schmidhuber, J. (1997). Long ShortTerm Memory. Neural Computation, 9, 1735â€“1780.
Long Short-Term Memory\cite{hochreiter_long_1997} (LSTM) is a type of Recurrent Neural Network (RNN) architecture that address the problem of vanishing or exploding gradients through time. This problem occurs in simple RNNs because the signal transmitted from one time-step to another is influenced at each interaction, both on forward and backward propagation phases, limiting the potential interaction between distant time-steps. The solution offered by LSTM introduces the concept of a memory cell, adding gates units to each node. The initial proposal included only input and output gates \cite{hochreiter_long_1997}.
The input gate is responsible for regulating if the cell internal state will be affected by the layer input. Similarly, the output gate regulates if the internal state will influence the output of the layer. The concept of a forget gate was introduced later \cite{gers_learning_1999}, to control whether the internal state of the cell should be reset.

The equations \eqref{eq:lstmequationsstart} to \eqref{eq:lstmequationsend} describe a typical LSTM network. At each time step $t$, $x_t$ is the input vector, $c_t$ is the cell's internal state and $h_t$ is the output state. Both $c_t$ and $h_t$ influence the next time step as variables $c_{t-1}$ and $h_{t-1}$. The symbol $\circ$ denotes a element wise multiplication.

The result of the forget, input, and output gates is represented by $f_t$, $i_t$, and $o_t$.
In each of the three gates, $W$ is the set of weights to be applied on the input $x$, while $U$ is the set of weights to be applied on the previous time step output, $h_{t-1}$. The dimensions of $x$ and $h_{t-1}$ are the chosen dimensions for input and output, while the dimension for $W$ and $U$ are such that the output has the same dimensions of $h_{t-1}$.  The symbol $b$ denotes the bias vector, having the same dimensions of both $h$ and $h_{t-1}$. The symbol $\sigma_g$ denotes the activation function of the gate. Each gate is mathematically equivalent to a fully connected layer where the inputs are the concatenation of the $x$ and $h_{t-1}$ vectors and the output has the same dimensions of $h$.

The activation function usually chosen for the gates is the sigmoid, which gives values in the $[0,1]$ range. If the forget gate results zero for a given component, the corresponding component in the $c_{t-1}$ vector will have no influence on $c_t$ or $h_t$. If all the resulting components of the forget gate are zero, which corresponds to the zero vector, then the previous cell's state $c_{t-1}$ is completely ignored.

Similarly, if the input gate results in a zero component, the corresponding component in the $c_t$ vector will not be influenced by $x$ and $h_{t-1}$.

If all components of $f_t$ and $i_t$ are equal to $1$, then $c_t = c_{t-1} + \sigma_c(x_t W_{c} + h_{t-1} U_{c} + b_c)$, which is the case where $c_t$ is fully influenced by $c_{t-1}$, $x$ and $h_{t-1}$.

The output gate follows a similar pattern, but it regulates how $h_t$ will be influenced by $c_t$

    \begin{align}
\label{eq:lstmequationsstart}     
f_t &= \sigma_g(x_t W_{f} + h_{t-1} U_{f} + b_f) \\
i_t &= \sigma_g(x_t W_{i} + h_{t-1} U_{i} + b_i) \\
o_t &= \sigma_g(x_t W_{o} + h_{t-1} U_{o} + b_o) \\
c_t &= f_t \circ c_{t-1} + i_t \circ \sigma_c(x_t W_{c} + h_{t-1} U_{c} + b_c) \\
\label{eq:lstmequationsend}
h_t &= o_t \circ \sigma_h(c_t)
    \end{align}


A python version of a LSTM time step calculation is presented in algorithm \ref{alg:keraslstm}. It is based on the Keras' implementation, but highly simplified.

Since the Keras framework is being used in this work, is worth mentioning that the default activation function for $\sigma_g$ in Keras' LSTM cell is the ``hard\_sigmoid'', which outputs $0$ if $x<-2.5$, $1$ if $x>2.5$ and $x*0.2 + 0.5$ if $x$ is between $-2.5$ and $2.5$.

Keras uses the same activation function for both $\sigma_c$ and $\sigma_h$, 
a pattern that has been retained in the algorithm \ref{alg:keraslstm}. The default $\sigma_c$ and $\sigma_h$ activation functions in Keras' LSTM cell implementation is the $\tanh$ function.

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left, caption={Simplified version of Keras' LSTM implementation},label={alg:keraslstm}]
h_tm1 = h
c_tm1 = c

z_i = K.dot(inputs, kernel_i) + K.dot(h_tm1, rec_kernel_i) + bias_i
z_f = K.dot(inputs, kernel_f) + K.dot(h_tm1, rec_kernel_f) + bias_f
z_c = K.dot(inputs, kernel_c) + K.dot(h_tm1, rec_kernel_c) + bias_c
z_o = K.dot(inputs, kernel_o) + K.dot(h_tm1, rec_kernel_o) + bias_o

i = recurrent_activation(z_i)
f = recurrent_activation(z_f)
c = f * c_tm1 + i * activation(z_c)
o = recurrent_activation(z_o)

h = o * activation(c)
\end{lstlisting}
\end{algorithm}
