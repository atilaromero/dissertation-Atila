The research described in this study was divided in three parts.
In chapter \ref{sec:evalmodels}, during the search for the best model to classify file fragments, an apparent limit on how far this models could be improved was found. This limitation can be also observed in other works, since training the model ``CLD'' with the same file types used in those works resulted in accuracy values similar to theirs.

In chapter \ref{sec:numberofclasses}, the influence of number of classes on accuracy was explored. It was observed that an increase of number of classes has the tendency of decreasing accuracy and increasing precision. But the number of classes alone was found to be less important than the type of extension selected: some file types when included in the experiment have a much higher negative impact than others, specially those that use compression or contain images.

In chapter \ref{sec:exprandom}, a method to measure entropy was proposed. One advantage of this method is that it can be customized to a particular neural network architecture. It was used to measure, for each file type, what portion of the data did not have structures detectable by the chosen model, ``CLD''.

This measure was used to verify the hypothesis that part of the errors observed on chapter \ref{sec:evalmodels} were caused by high entropy on some file types, as the results of chapter  \ref{sec:numberofclasses} suggested.

The portion of data that could be identified as containing structures was higher than expected. While some of the errors could be explained by the inability of the models to distinguish  high entropy data from random data, this could only explain about 1/3 of the observed errors (12.7\% out of 37\%).

This raises the question of whether the remaning 2/3 of errors could be explained by different file types using the same data structures. This error source comes from the practice of using the extension of the file as the class to each of its parts. An analogy with speech recognition would be to label each syllable of a spoken word using the word as its label and then try to predict the whole word using only a syllable.

Unfortunately, for the file fragmentation task, the potential labels for smaller parts may be less obvious than it is for speech recognition. The best-case scenario would be if the neural network itself could choose the labeling. But evaluating those predicted labels using labels of its own choosing would introduce bias, as it could prefer to create only easy labels.
% The statement that all file types are composed of ``data'' is correct, but it is unhelpful.

An additional contribution of this work is the availability of the source code used to generate the results, which is a feature that not all studies provide. It can be used as a basis of comparison in future researches, generating models with the same architecture of those presented here.