
File fragment classification aims to identify the original type of file from which a given block of data was extracted. Machine learning techniques, and neural networks in particular, have the potential to improve this field, because the support of a new file type using traditional methods is laborious and not automatic. While studies on neural networks applied for file fragment classification have shown good results, to apply those contributions on real scenarios a better understanding is required on how those methods respond to an increase in number of supported file types and what the are the main sources of errors of this approach.

The research described in this study was divided into three parts.
In Chapter \ref{sec:evalmodels}, during the search for the best model to classify file fragments, an apparent limit on how far these models could be improved was found. This limitation can be also observed in other works, since training the model ``CLD'' with the same file types used in those works resulted in accuracy values similar to theirs.

In Chapter \ref{sec:numberofclasses}, the influence of the number of classes on accuracy was explored. It was observed that an increase in the number of classes tends to decrease the accuracy and to increase the precision. But the number of classes alone was found to be less important than the type of extension selected: some file types when included in the experiment have a much higher negative impact than others, especially those that use compression or contain images.

In Chapter \ref{sec:exprandom}, a method to measure entropy was proposed. One advantage of this method is that it can be customized to a particular neural network architecture. It was used to measure, for each file type, what portion of the data did not have structures detectable by the chosen model, ``CLD''.

This measure was used to verify the hypothesis that part of the errors observed in Chapter \ref{sec:evalmodels} were caused by high entropy on some file types, as the results of Chapter  \ref{sec:numberofclasses} suggested.

The portion of data that could be identified as containing structures was higher than expected. While some of the errors could be explained by the inability of the models to distinguish  high entropy data from random data, this could only explain about 1/3 of the observed errors (12.7\% out of 37\%).

This raises the question of whether the remaining 2/3 of errors could be explained by different file types using the same data structures. This error source comes from the practice of using the extension of the file as the class to each of its parts. An analogy with speech recognition would be to label each syllable of a spoken word using the word as its label and then try to predict the whole word using only a syllable.

This is an important contribution because using the extension of the file as the fragment label is common practice in studies in this field. The inefficiency of this labeling system may have been overlooked because the accuracy errors were likely attributed to the existence of high entropy data. Since high entropy data can only explain a part of the misclassification errors, as this study showed in Chapter 6, the assumption that the search for a better model would be the best approach to improve file fragment classification results may be wrong because the labeling system should be reviewed first. It is possible that existing models already reached the limit of what can be done without a revision in the labeling system, as was shown in Chapter 4, where the ``CLD'' model achieved results similar to other studies.

Unfortunately, in the file fragmentation task the potential labels for smaller parts may be less obvious than it is in speech recognition. The best-case scenario would be if the neural network itself could choose the labeling. But evaluating those predicted labels using labels of its own choosing would introduce bias, as it could prefer to create only easy labels.
% The statement that all file types are composed of ``data'' is correct, but it is unhelpful.

An additional contribution of this work is the availability of the source code used to generate the results, which is a feature that not all studies provide. It can be used as a basis of comparison in future researches, generating models with the same architecture of those presented here.