\section{Framework}
TensorFlow and Keras are the main parts of the environment used to develop and test the neural network models considered in this work.

TensorFlow \cite{abadi_tensorflow:_2016} is a open-source machine learning framework that uses dataflow graphs to represent computation and state in a symbolic form. Its basic unit, the tensor, is a multidimensional array. In the case of neural networks, the use of dataflow graphs to represent forward propagation allows TensorFlow to infer the partial derivatives needed to construct the backpropagation phase, relieving the user from manually performing this task.

Performance, scalability and easy of use were main concerns during TensorFlow design. It can use CPUs, GPUs and TPUs, runs on several operating systems and in different architectures.

As described in its site, https://keras.io/\cite{chollet_keras_2019}, ``Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.'' In recent versions, TensorFlow includes Keras in its core as a python module, and TensorFlow documentation and guides present Keras as the first option when learning TensorFlow.

Keras simplifies the creation of layers and models, and standardize how they can be saved and shared. Instead of specifying each mathematical operation required to build a layer, using Keras the user would instantiate a layer of the desired type. This approach facilitates experimentation with different types of neural network models, since the addition of a layer can be done in a single line of code, and with reduced chance of bugs.


\subsection{Environment}

There are several options on how to build an environment that uses TensorFlow and Keras. For fast experimentation, Google's Colaboratory\cite{carneiro_performance_2018} is among the most practical solutions. Since it is processed remotely in Google's infrastructure, it can be accessed via browser, providing choices for CPU, GPU or TPU usage, without any installation requirement.

But in many circumstances a local installation is preferable. It may be desirable for example to perform benchmark with a specific hardware, or to use unit tests, or to execute long training sessions. 
During the assemble of a local TensorFlow installation, the first choice to make is whether to use GPU support or not. A CPU-only install is easier, while a GPU capable one will provide better performance.

Two good options for a CPU-only TensorFlow install are installation via pip or via docker. The docker option is slightly more complex but gives better reproducibility. They are otherwise equivalent.

TensorFlow with GPU support requires the installation of CUDA and cuDNN. Since the available GPU hardware may require specific versions of CUDA and cuDNN, it may be necessary to install TensorFlow from sources instead of using pre-compiled versions.
% {cuda9.0,9.2,10, cuDNN7.4.2}

Most of the development done on the current work was made using a CPU-only version of TensorFlow inside a docker container. The tests and experiments performed were organized in folders that reflect the history of these trials. The resulting code increases redundancy, since in many cases the differences between two similar folders are minimal, a change in the number of units of a layer for example, but facilitates comparison between them. The initial approach of relying only on git history to keep this information was not practical as it would not provide the same ease of comparison.

The repository containing the tests and experiments is available at \url{http://github.com/atilaromero/ML}. The file ``Dockerfile'' can be used to create an environment with the required dependencies installed. Within most experiment folders there is a ``main.py'' file that runs the experiment, usually a training procedure. In other cases, python unit tests where used.
