Having recognized the potential of neural networks in the data carving task, there are still some research paths that should be explored.

One of them is about the increase in the range of supported filetypes. In a collaborative approach, should the said community be sharing models, datasets, or both? What are the strengths and weakness of each?

Another one is about reassembling. After each block has been classified, how to reconstruct the original file in the occurrence of fragmentation?

The last one is about the recognized structures. Is it possible to use the trained networks to describe the structures being recognized?

% The following questions are intended to be answered in future works: 

% \begin{enumerate}[itemindent=\parindent,label=\textbf{Q\arabic*.}]

%     \item Could a neural network based tool support a wider range of file types?
    
%     \item Could a neural network based tool handle fragmentation through reassembling?
    
%     % \item Do the results obtained with usual datasets reflect what happens in real scenarios?

% \item Do neural networks help to interpret internal file structures?

% \end{enumerate}

% \todo[inline]{compare solutions - possible candidates: feedforward, convolutional, LSTM, BLSTM, SVM, kNN, Photorec, Foremost, scalpel}
% \todo[inline]{shuffle data to simulate fragmentation}
% \todo[inline]{removal of portions of files to simulate data corruption}
% \todo[inline]{increase the number of supported file types, investigating the best strategy to scale the solution}
% \todo[inline]{reassembling}
% \todo[inline]{model share}
% \todo[inline]{adaption of visualization techniques of neural networks, attempting to infer file structure.}