\section{\label{sec:sequencelabeling}Sequence labeling}

%Sequence labeling was the algorithm class selected to be further studied, as it considers relations between labels on a sequence, which is often the case when classifying consecutive sectors of a disk. This could lead to better classification and to reassembling of fragmented data.

%Sequence labeling deals with the task of attributing categorical labels to a group of instances where, unlike happens with other classification problems, those instance labels are not independent.
%One example of such task is known as Part Of Speech (POS) tagging, where the goal is, given a word in a sentence, classify it, for example as a noun or a verb.
%The classification of each word is not independent of its surroundings since it provides context from which the meaning of the word may be inferred.

%Another example of a sequence labeling task is found in speech recognition.
%The sound of a spoken sentence is split into parts and each part receives a label corresponding to a phoneme. But those labels are not independent since some combinations of phonemes are meaningful while others are not. Thus, taking this context into account increases the accuracy of the results.


\todo[inline]{What is sequence labeling}

\subsection{\label{sec:lstm}Long Short-Term Memory}
% Hochreiter schmidhuber, 1997
% Hochreiter, S., & Schmidhuber, J. (1997). Long ShortTerm Memory. Neural Computation, 9, 1735â€“1780.
\todo[inline]{What is LSTM}

\subsection{\label{sec:ctc}Connectionist Temporal Classification}
% Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks


\todo[inline]{What is CTC}
