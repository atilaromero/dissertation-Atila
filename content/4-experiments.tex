% \levelC{Dataset}
This study uses the Govdocs1 dataset \cite{garfinkel_bringing_2009}, which was fully downloaded and its files were grouped by extension. This dataset has files with 63 different extensions. The 33 extensions with less than 200 files were discarded. The  ``text'' and ``unk'' extensions were discarded because files with these extensions use multiple formats and they do not correspond to a single file type. From the remaining 28 extensions, listed in table \ref{tab:govdocs1}, 200 files of each were randomly selected, 100 to use in the training dataset and 100 to use in the validation dataset.

\input{content/tables/4.0.1-govdocs.tex}

% \levelC{Hardware}
The experiments did not take advantage of GPU acceleration and were  conducted on a single computer with 256GB of RAM and with 2 Intel\textregistered Xeon\textregistered E5-2630 v2 processors, with 6 cores each, with 2 hyper-threads per core, or 24 hyper-threads in total. 


% \levelC{Software}
The main software and frameworks used to build the experiments were Python 3.6, Jupyter notebook, Tensorflow 1.14.0, Keras 2.2.4-tf, and Fedora Linux 27.

% repository
The source code for the experiments is available at \sloppy\url{http://github.com/atilaromero/randomness-experiments}.

Section \ref{sec:evalmodels} evaluates some alternative models in the file fragment classification task. The expectation was to identify the most promising models for improvement. Instead, an apparent limit was found on how far these models could be improved. 

Section \ref{sec:numberofclasses} describes how does the accuracy of neural network models change relative to the number of classes. It was observed that high accuracy in the file fragment classification task could be achieved only when the number of classes was small. Also, when compared with each other, file types with high entropy data showed the lowest accuracy values.

Section \ref{sec:exprandom} investigates the hypothesis that some part of these errors may be explained by the inability of the models to distinguish high entropy data from random data. The portion of data that could be identified as not random was higher than expected. While some of the errors could have the explanation mentioned in the hypothesis, it could only explain about 1/3 of the observed errors (16.5\% out of 46\%) \todo{revise numbers}.

\input{content/4.0.1-researchonmodels.tex}

\input{content/4.0.2-researchonnumberofclasses.tex}

\input{content/4.0.3-experimentrandomdata.tex}

