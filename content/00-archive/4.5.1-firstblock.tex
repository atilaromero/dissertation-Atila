\todo[inline] {deixar mais claro antes que os experimentos est√£o organizados desta forma}
\todo[inline] {aumentar fonte nas bordas das figuras}

In the first set of experiments, for each file selected as input sample from the test dataset, only the first 512 bytes of that file were used. For most filetypes, including those three under consideration, these first bytes contains regular patterns in fixed positions, which makes them easy to recognize.

Each epoch was configured to draw 100 samples from the training dataset. Validation was performed using 100 samples from the development dataset. Each sample had only 512 bytes.

Three network configurations were compared. The simplest one was a feedforward network, referred as ``D'', with only a single fully connected layer, with 131072 input units (512 bytes times 256 possible values using one hot encoding) and 3 output units, one for each output class, PDF, JPG, or PNG.

Another one, referred as ``LD'', used a LSTM layer with 32 output units, followed by a fully connected layer of 3 output units. The LSTM layer process one byte per time step. This is one of the simplest LSTM configurations. While it would be possible to use only a single LSTM layer without a fully connected layer next, then the LSTM layer would have to be limited to 3 output units, and would have too few trainable parameters. 

The last one, referred as ``CL'', used a convolutional layer without max pooling with 32 input units, 3 output units, and a stride of 32 followed by a LSTM layer with 3 output units. Some other convolution sizes and strides were tried during the test development. A convolutional layer with 512 units of input would be equivalent to a fully connected layer, as no convolution would be performed. Using 32 input units and a stride of same size, the convolutional layer breaks the block in 16 parts with 32 bytes each. The convolutional layer already outputs the exact number of classes, thus the LSTM layer here is just summarizing the results.

Table \ref{tab:carving7-11} gives a summary of the results of this set of experiments, listing for each model the number of trainable parameters, if the training used all of the blocks of the sample file or just the first one, the number of training epochs, the duration of the training, and the final accuracy on the training and on the validation datasets.

\begin{table*}[!ht]
    \centering
    \caption{Filetype identification experiments with first block}
    \label{tab:carving7-11}
\begin{tabular}{r|r|r|r|r|r|r}
\hline
Name & Parameters & Blocks & Epochs & Time    & Training          & Validation          \\       
     &            &        &        &         &          accuracy &            accuracy \\ \hline\hline

D	& 393219	& first	& 1	    & 0m01s	& 0.91	& 1.0 \\ \hline
LD	& 37091	    & first	& 28	& 2m54s	& 0.92	& 0.68 \\ \hline
CL	& 24663	    & first	& 4	    & 0m05s	& 0.98	& 0.73 \\ \hline
\end{tabular}
\end{table*}

The simple feedforward network was the fastest networking concerning training time, and had the best validation accuracy. It is interesting to notice that the LSTM without a previous convolutional network had a slow training time in comparison.

Since the simplest network was giving the best results, the next experimentation step was to check if this performance would be maintained in a more complex scenario.

