\section{Objective}
The objective of the experiments described in this section 
was to compare the training time of different neural networks, given an target accuracy level, at the filetype identification task.

\section{Methodology}

The input of the network for each instance is a 512x256 matrix representing only one block of 512 bytes of a random file of the dataset. Each of the 512 bytes is one-hot encoded, meaning that its value is converted into a vector with 256 elements, with only one of them set to 1, corresponding to the value of the byte, while the others are set to zero.

Each epoch was configured to draw 1000 samples from the training dataset. Validation was performed using the development dataset, with another 300 files and and also using only the first 512 bytes.


The output of the network for a given instance is a vector with 3 elements, subjected to a softmax function, which applies the exponential function on the vector and then normalizes it. Each value will represent the predicted probability that the instance belongs to each of the three classes, PDF, JPG or PNG.

Minimize the training time is important when the task of create new models to new filetypes is delegated to the forensic examiners community, since they may not have expensive hardware or time to train a more demanding neural network.

For similar reasons, networks that require too much tuning to perform well are undesirable, since it would require a higher knowledge and work from the forensic examiner.


%metricas
To identify networks with such requirements, each experiment use two stop conditions, a limit of 150 epochs or an accuracy of 90\% in the training set, whichever occurs first. These limits were established during the first preliminary results.
%Since the rate of change of accuracy may present oscillations during training, a visual comparison of accuracy versus time curves is important to confirm which network has better training times.

All the experiments use the Adam
%%%%%
\todo{reference}
%https://arxiv.org/abs/1412.6980v8
%https://openreview.net/forum?id=ryQu7f-RZ
optimization algorithm to guide backpropagation, which was selected because it performed well in the preliminary results without requiring tuning the learning rate.

%modelos
The networks considered used different combinations of convolutional, maxpooling, LSTM, and fully connected layers.


\section{Datasets}

Datasets for three different file types, JPG, PDF, and PNG, were obtained from online sources.

The PDF files were retrieved from http://arxiv.org/pdf/, which stores them using a sequential nomenclature. The training dataset uses the files 1904.10000.pdf to 1904.10099.pdf, development 1904.10100.pdf to 1904.10199.pdf, and test 1904.10200.pdf to 1904.10299.pdf. 

The JPG samples were obtained in ftp://ftp.inrialpes.fr/pub/lear/douze/data/jpg1.tar.gz \todo{reference}, and 100 images were assigned to training, 100 to development and the remaining 612 to test.
%If you use this dataset, please cite the following paper:
% Herve Jegou, Matthijs Douze and Cordelia Schmid
% "Hamming Embedding and Weak geometry consistency for large scale image search"
% Proceedings of the 10th European conference on Computer vision, October, 2008

The PNG images were obtained from \url{http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip}\todo{reference}, and 100 images were assignet to training, 100 to development and the remaining 700 to test.

%dataset paper: http://www.vision.ee.ethz.ch/~timofter/publications/Agustsson-CVPRW-2017.pdf
% @InProceedings{Agustsson_2017_CVPR_Workshops,
% author = {Agustsson, Eirikur and Timofte, Radu},
% title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
% booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
% month = {July},
% year = {2017}
% }

\section{Environment}
The experiments did not take advantage of GPU acceleration and were  conducted on a single computer with 256GB of RAM and with 2 Intel\textregistered Xeon\textregistered E5-2630 v2 processors, with 6 cores each, with 2 hyper-threads per core, or 24 hyper-threads in total. 

The source code for the experiments is available at \url{http://github.com/atilaromero/ML}.

\section{Results}

% \subsection{Experiments 1 to 6}

% Experiments 1 to 6 use a limited dataset, with only 3 files as source. Each input to the network is a random sector from one of these files. During these first tests, different strategies to structure the tests were being tried, and that was their main contribution.

% Table \ref{tab:carving1-6} gives a summary of the results of experiments 1 to 6. The ``C'', ``M'', ``L'', and ``D'' columns indicate the number of convolutional, maxpooling, LSTM and dense layers in each model structure.
% \input{content/5.3-z-tab-carving1-6.tex}

% Experiment 1 uses a convolutional layer of 16 units followed by a maxpooling layer, a LSTM layer of 16 units and a dense layer of 3 units.

% Experiment 2 increases the number of convolutional layers, but it was unable to finish the training in less than 150 epochs.

% Experiment 3 introduces the use of the ``fit\_generator'' function, which makes the sampling sectors from the training files easier, avoid manual counting of the epochs, uses callbacks, and uses the development dataset in validation.

% Experiment 4 skips the initialization with ones that was being used in the LSTM layer at the previous experiments as an attempt to increase reproducibility. It also introduces periodic saving of TensorBoard data.

% Experiment 5 skips the convolutional layer, but the resulting network was too slow to train.

% Experiment 6 skips the maxpooling layer.

% \noindent
% \begin{algorithm}
% \begin{lstlisting}[language=Python, frame=single, numbers=left]
% last = l0 = tf.keras.layers.Input(shape=(512,256))
% last = Conv1D(16, (4,), padding="same", activation="relu")(last)
% last = MaxPooling1D(pool_size=(2,))(last)
% last = LSTM(16, return_sequences=False, dropout=0.5,
%             kernel_initializer=tf.keras.initializers.Ones())(last)
% last = Dense(3)(last)
% last = Activation('softmax')(last)

% model = tf.keras.Model([l0], last)
% \end{lstlisting}
% \caption{\label{alg:3-files-001}Experiment 3-files-001}
% \end{algorithm}


\subsection{First block as input}
In the first set of experiments, for each file selected as input sample from the test dataset, only the first 512 bytes of that file were used. For most filetypes, including those three under consideration, these first bytes contains regular patterns in fixed positions, which makes them easy to recognize.



Three network configurations were compared. The simplest one was a feedforward network, with only a single fully connected layer, with 131072 input units (512 bytes times 256 possible values using one hot encoding) and 3 output units, one for each output class, PDF, JPG, or PNG.

Another used a LSTM layer with 16 output units, using a dropout of 0.5, followed by a fully connected layer of 3 output units.

The last one used a convolutional layer without maxpooling with 32 input units, 3 output units, and a stride of 16 followed by a LSTM layer with 3 output units. Some other convolution sizes and strides were tried during the test development. A convolutional layer with 512 units of input would be equivalent to a fully connected layer, as no convolution would be performed.

Table \ref{tab:carving7-11} gives a summary of the results of experiments 7 to 11. The ``C'', ``L'', and ``D'' columns indicate the number of convolutional, LSTM and dense layers in each model structure.
\input{content/5.3-z-tab-carving7-11.tex}

The simple feedforward network was by far the fastest networking concerning training time, and was able to reach 100\% accuracy in only one epoch. It is interesting to notice that the LSTM without a previous convolutional network had a slow training time.



%%%%%%%%%%%%%%%%%%%%%%%

% All experiments of this set were able to reach the desired accuracy of 90\% in a few epochs. This can be explained by the regularity in the structure which is a characteristic of the beginning of the files.

% Table \ref{tab:carving7-11} gives a summary of the results of experiments 7 to 11. The ``C'', ``R'', ``L'', and ``D'' columns indicate the number of convolutional, recurrent, LSTM and dense layers in each model structure.
% \input{content/5.3-z-tab-carving7-11.tex}

% Experiment 7 uses almost the same code of experiment 5, it is a single LSTM layer with 0.5 dropout and a output fully connected layer of 3 units, one for each filetype in consideration, PDF, JPG, and PNG.

% Experiment 8 uses four RNN layers followed by a LSTM layer.

% Experiment 9 is similar to the previous one, but uses only 1 RNN instead of 4.

% Experiment 10 is the simplest network, only a fully connected layer with 131072 input units (512 bytes times 256 possible values using one hot encoding) and 3 output units. It was by far the fastest networking concerning training time, and was able to reach the desired accuracy in only one epoch.

% Experiment 11 uses a convolutional layer without maxpooling with 32 input units, 3 output units, and a stride of 16 followed by a LSTM layer with 3 output units. Some other convolution sizes and strides were tried during the test development. A convolutional layer with 512 units of input would be equivalent to a fully connected layer, as no convolution would be performed.

\subsection{Random block as input}

The next set of experiments use a random block (512 bytes chunks) from each file, instead of just the first one. This is a harder classification task because, while in the first block is reasonable to find patterns in specific positions in relation to the beginning of the block, this correspondence is not normally preserved in the remaining blocks, the pattern may start anywhere in the block. The second factor is that in files with low compression rates, as image files, the beginning of the file normally presents more recognizable patterns than the middle.


XXXX network configurations are here compared.
The best group of results are composed by two network configurations that use two convolutional layers with a maxpooling followed by one or two LSTM layers. The results for using one or two LSTM layers were very similar, taking 33 and 30 minutes each, with 88.9\% and 88.6\% of accuracy in the validation dataset. 

The simple feedforward network that performed well classifying the first block was unable to achieve similar results when classifying a random block. In 600 epochs, it only reached an accuracy of 77.5\% on the validation dataset, taking 62 minutes.

%%%% 13 23 16 15
The second best group of results was achieved by four networks with similar characteristics. One of then uses a convolutional layer followed by a LSTM layer, and the other three are variations of it, by adding a second LSTM layer, or by adding a fully connected layer at the end, or by adding maxpooling to the convolutional layer. These four networks presented similar results.


\todo[inline]{here}
Three networks were 
%%%% 25
Experiment 25 uses a convolutional layer followed by a fully connected layer.
%%%%
%%%% 26 e 27
Experiments 25, 26, and 27 do not use LSTM. Experiment 26 uses a convolutional layer followed by a maxpooling layer. 
Experiment 27 uses 2 convolutional layers and a maxpooling layer.
%%%%
Table \ref{tab:carving12-27} gives a summary of the results of this set of experiments. The ``C'', ``M'', ``L'', and ``D'' columns indicate the number of convolutional, maxpooling, LSTM and dense layers in each model structure.
\input{content/5.3-z-tab-carving12-27.tex}

