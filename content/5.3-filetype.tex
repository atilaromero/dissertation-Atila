\section{Filetype identification}
The objective of the experiments described in this section was to investigate which neural network models would be more adequate to the filetype identification task and to be used in subsequent research stages.
\subsection{Datasets}

Datasets for three different file types, JPG, PDF, and PNG, were obtained from online sources.

The PDF files were retrieved from http://arxiv.org/pdf/, which stores them using a sequential nomenclature. The training dataset uses the files 1904.10000.pdf to 1904.10099.pdf, development 1904.10100.pdf to 1904.10199.pdf, and test 1904.10200.pdf to 1904.10299.pdf. 

The JPG samples were obtained in ftp://ftp.inrialpes.fr/pub/lear/douze/data/jpg1.tar.gz \todo{reference}, and 100 images were assigned to training, 100 to development and the remaining 612 to test.
%If you use this dataset, please cite the following paper:
% Herve Jegou, Matthijs Douze and Cordelia Schmid
% "Hamming Embedding and Weak geometry consistency for large scale image search"
% Proceedings of the 10th European conference on Computer vision, October, 2008

The PGN images were obtained from \url{http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip}\todo{reference}, and 100 images were assignet to training, 100 to development and the remaining 700 to test.

%dataset paper: http://www.vision.ee.ethz.ch/~timofter/publications/Agustsson-CVPRW-2017.pdf
% @InProceedings{Agustsson_2017_CVPR_Workshops,
% author = {Agustsson, Eirikur and Timofte, Radu},
% title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
% booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
% month = {July},
% year = {2017}
% }

\subsection{Results}

\input{content/5.3-z-tab-carving1-6.tex}




% def compile(model):
%     model.compile(loss=tf.keras.losses.categorical_crossentropy,
%         optimizer=tf.keras.optimizers.Adam(),
%         metrics=['accuracy'])

\todo[inline]{describe identification experiments}
\subsubsection{3-files-001}

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = tf.keras.layers.Input(shape=(512,256))
last = Conv1D(16, (4,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=(2,))(last)
last = LSTM(16, return_sequences=False, dropout=0.5,
            kernel_initializer=tf.keras.initializers.Ones())(last)
last = Dense(3)(last)
last = Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-001}Experiment 3-files-001}
\end{algorithm}

\subsubsection{3-files-002}

%channels_first
%return_sequences=True
% explain necessity of LSTM

% _________________________________________________________________
% Layer (type)                 Output Shape              Param #   
% =================================================================
% input_1 (InputLayer)         (None, 512, 256)          0         
% _________________________________________________________________
% conv1d (Conv1D)              (None, 512, 128)          98432     
% _________________________________________________________________
% max_pooling1d (MaxPooling1D) (None, 512, 64)           0         
% _________________________________________________________________
% conv1d_1 (Conv1D)            (None, 512, 64)           12352     
% _________________________________________________________________
% max_pooling1d_1 (MaxPooling1 (None, 512, 32)           0         
% _________________________________________________________________
% conv1d_2 (Conv1D)            (None, 512, 32)           3104      
% _________________________________________________________________
% max_pooling1d_2 (MaxPooling1 (None, 512, 16)           0         
% _________________________________________________________________
% conv1d_3 (Conv1D)            (None, 512, 16)           784       
% _________________________________________________________________
% max_pooling1d_3 (MaxPooling1 (None, 512, 8)            0         
% _________________________________________________________________
% conv1d_4 (Conv1D)            (None, 512, 8)            200       
% _________________________________________________________________
% max_pooling1d_4 (MaxPooling1 (None, 512, 4)            0         
% _________________________________________________________________
% lstm (LSTM)                  (None, 512, 16)           1344      
% _________________________________________________________________
% lstm_1 (LSTM)                (None, 16)                2112      
% _________________________________________________________________
% dense (Dense)                (None, 3)                 51        
% _________________________________________________________________
% activation (Activation)      (None, 3)                 0         
% =================================================================
% Total params: 118,379
% Trainable params: 118,379
% Non-trainable params: 0

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = Input(shape=(512,256))
last = Conv1D(128, (3,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=2, strides=2, data_format='channels_first')(last)
last = Conv1D(64, (3,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=2, strides=2, data_format='channels_first')(last)
last = Conv1D(32, (3,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=2, strides=2, data_format='channels_first')(last)
last = Conv1D(16, (3,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=2, strides=2, data_format='channels_first')(last)
last = Conv1D(8, (3,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=2, strides=2, data_format='channels_first')(last)
last = LSTM(16, return_sequences=True)(last)
last = LSTM(16, return_sequences=False)(last)
last = Dense(3)(last)
last = Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-002}Experiment 3-files-002}
\end{algorithm}

uses a fit\_generator function
interrompido, demorava muito para terminar

\subsubsection{3-files-003}

same model of 3-files-001, but using fit\_generator
Adam, categorical\_crossentropy

training data: just 3 files (source?)
validation data: dev

% train = utils.load.examples_from('../../datasets/carving/3files')
% validation = utils.load.examples_from('../../datasets/carving/dev')

31 epochs
\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = tf.keras.layers.Input(shape=(512,256))
last = Conv1D(16, (4,), padding="same", activation="relu")(last)
last = MaxPooling1D(pool_size=(2,))(last)
last = LSTM(16, return_sequences=False, dropout=0.5,
            kernel_initializer=tf.keras.initializers.Ones())(last)
last = Dense(3)(last)
last = Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-003}Experiment 3-files-003}
\end{algorithm}

\subsubsection{3-files-004}

almost same code of 3-files-003
dont use 1's initializer in LSTM
add tensorboard

46 epochs

\subsubsection{3-files-005}

almost same code of 3-files-004
remove convolutional layer and maxpooling

interrompido??

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
\end{lstlisting}
\caption{\label{alg:3-files-005}Experiment 3-files-005}
\end{algorithm}

\subsubsection{3-files-006}

almost same code of 3-files-004
remove maxpooling layer

interrompido??

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
\end{lstlisting}
\caption{\label{alg:3-files-006}Experiment 3-files-006}
\end{algorithm}

\subsubsection{3-files-007}

almost same code of 3-files-005 (no conv, no maxpooling)
uses only first sectors
uses ../../datasets/carving/train

21 epochs

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
\end{lstlisting}
\caption{\label{alg:3-files-007}Experiment 3-files-007}
\end{algorithm}

\subsubsection{3-files-008}
uses four RNN layers followed by a LSTM layer
on\_epoch\_end was using \%5 before, removed and now save and acc<0.9 is check every time
too slow

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = tf.keras.layers.Input(shape=(None,256))
last = SimpleRNN(128, return_sequences=True, dropout=0.1)(last)
last = SimpleRNN(64, return_sequences=True, dropout=0.1)(last)
last = SimpleRNN(32, return_sequences=True, dropout=0.1)(last)
last = SimpleRNN(16, return_sequences=True, dropout=0.1)(last)
last = LSTM(8, return_sequences=False, dropout=0.1)(last)
last = Dense(3)(last)
last = Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-008}Experiment 3-files-008}
\end{algorithm}

\subsubsection{3-files-009}

based on previous, but only 1 RNN instead of 4
5 epochs

\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
\end{lstlisting}
\caption{\label{alg:3-files-009}Experiment 3-files-009}
\end{algorithm}

\subsubsection{3-files-010}

flatten + dense

%validation_steps=100,
%steps_per_epoch=1000,

super fast

% _________________________________________________________________
% Layer (type)                 Output Shape              Param #   
% =================================================================
% input_1 (InputLayer)         (None, 512, 256)          0         
% _________________________________________________________________
% flatten (Flatten)            (None, 131072)            0         
% _________________________________________________________________
% dense (Dense)                (None, 3)                 393219    
% _________________________________________________________________
% activation (Activation)      (None, 3)                 0         
% =================================================================
% Total params: 393,219
% Trainable params: 393,219
% Non-trainable params: 0
% _________________________________________________________________
% Epoch 1/1000
% 100/100 [==============================] - 1s 12ms/step - loss: 0.0162 - acc: 1.0000   
% 1000/1000 [==============================] - 14s 14ms/step - loss: 0.0168 - acc: 0.9980 - val_loss: 0.0162 - val_acc: 1.0000



\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = tf.keras.layers.Input(shape=(512,256))
last = tf.keras.layers.Flatten()(last)
last = tf.keras.layers.Dense(3)(last)
last = tf.keras.layers.Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-010}Experiment 3-files-010}
\end{algorithm}

\subsubsection{3-files-011}


first sector
testing convolution sizes and strides
using size 512 is equivalent to flatten + dense

% _________________________________________________________________
% Layer (type)                 Output Shape              Param #   
% =================================================================
% input_1 (InputLayer)         (None, 512, 256)          0         
% _________________________________________________________________
% conv1d (Conv1D)              (None, 31, 3)             24579     
% _________________________________________________________________
% lstm (LSTM)                  (None, 3)                 84        
% _________________________________________________________________
% activation (Activation)      (None, 3)                 0         
% =================================================================
% Total params: 24,663
% Trainable params: 24,663
% Non-trainable params: 0
% _________________________________________________________________
% Epoch 1/1000
% 100/100 [==============================] - 1s 15ms/step - loss: 0.2851 - acc: 0.9930    
% 1000/1000 [==============================] - 45s 45ms/step - loss: 0.4344 - acc: 0.9547 - val_loss: 0.2851 - val_acc: 0.9930

% real	0m49.367s
% user	2m41.785s
% sys	0m48.219s


\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = tf.keras.layers.Input(shape=(512,256))
last = tf.keras.layers.Conv1D(3, (32,), strides=16)(last)
last = tf.keras.layers.LSTM(3)(last)
last = tf.keras.layers.Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-011}Experiment 3-files-011}
\end{algorithm}

\subsubsection{3-files-012}

based on previous, use sample\_sector instead of first\_sector

did not reach 0.9, interrupted on 281 epochs

% Epoch 280/1000
% 100/100 [==============================] - 1s 10ms/step - loss: 2.1047 - acc: 0.7490
% 1000/1000 [==============================] - 6s 6ms/step - loss: 1.5175 - acc: 0.8150 - val_loss: 2.1047 - val_acc: 0.7490


\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
\end{lstlisting}
\caption{\label{alg:3-files-012}Experiment 3-files-012}
\end{algorithm}


\subsubsection{3-files-013}

sample sector, model

% Epoch 134/1000
% 100/100 [==============================] - 1s 13ms/step - loss: 0.4387 - acc: 0.8900
% 1000/1000 [==============================] - 39s 39ms/step - loss: 0.4144 - acc: 0.9020 - val_loss: 0.4387 - val_acc: 0.8900

% real	84m37.671s
% user	385m11.711s
% sys	98m3.540s


\noindent
\begin{algorithm}
\begin{lstlisting}[language=Python, frame=single, numbers=left]
last = l0 = tf.keras.layers.Input(shape=(512,256))
last = tf.keras.layers.Conv1D(32, (32,), strides=32)(last)
last = tf.keras.layers.LSTM(64, return_sequences=True)(last)
last = tf.keras.layers.LSTM(3)(last)
last = tf.keras.layers.Activation('softmax')(last)

model = tf.keras.Model([l0], last)
\end{lstlisting}
\caption{\label{alg:3-files-013}Experiment 3-files-013}
\end{algorithm}

