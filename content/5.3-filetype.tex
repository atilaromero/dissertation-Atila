\section{Objective}
The objective of the experiments described in this section 
was to compare the training time of different neural networks, given an target accuracy level, at the filetype identification task.

\section{Methodology}

The input features of the network for each instance is a 512x256 matrix representing only one block of 512 bytes of a random file of the dataset. Each of the 512 bytes is one-hot encoded, meaning that its value is converted into a vector with 256 elements, with only one of them set to 1, corresponding to the value of the byte, while the others are set to zero.

Each epoch was configured to draw 100 samples from the training dataset. Validation was performed using 100 samples from the development dataset and and also using only 512 bytes.

The output of the network for a given instance is a vector with 3 elements, subjected to a softmax function, which applies the exponential function on the vector and then normalizes it. Each value will represent the predicted probability that the instance belongs to each of the three classes, PDF, JPG or PNG.

Minimize the training time is important when the task of create new models to new filetypes is delegated to the forensic examiners community, since they may not have expensive hardware or time to train a more demanding neural network.

For similar reasons, networks that require too much tuning to perform well are undesirable, since it would require a higher knowledge and work from the forensic examiner.


%metricas
To identify networks with such requirements, each experiment use two stop conditions, a limit of 150 epochs or an accuracy of 90\% in the training set, whichever occurs first. These limits were established during the first preliminary results.
%Since the rate of change of accuracy may present oscillations during training, a visual comparison of accuracy versus time curves is important to confirm which network has better training times.

All the experiments use the Adam
%%%%%
\todo{reference}
%https://arxiv.org/abs/1412.6980v8
%https://openreview.net/forum?id=ryQu7f-RZ
optimization algorithm to guide backpropagation, which was selected because it performed well in the preliminary results without requiring tuning the learning rate.

%modelos
The networks considered used different combinations of convolutional, maxpooling, LSTM, and fully connected layers.


\section{Datasets}

Datasets for three different file types, JPG, PDF, and PNG, were obtained from online sources.

The PDF files were retrieved from http://arxiv.org/pdf/, which stores them using a sequential nomenclature. The training dataset uses the files 1904.10000.pdf to 1904.10099.pdf, development 1904.10100.pdf to 1904.10199.pdf, and test 1904.10200.pdf to 1904.10299.pdf. 

The JPG samples were obtained in ftp://ftp.inrialpes.fr/pub/lear/douze/data/jpg1.tar.gz \todo{reference}, and 100 images were assigned to training, 100 to development and the remaining 612 to test.
%If you use this dataset, please cite the following paper:
% Herve Jegou, Matthijs Douze and Cordelia Schmid
% "Hamming Embedding and Weak geometry consistency for large scale image search"
% Proceedings of the 10th European conference on Computer vision, October, 2008

The PNG images were obtained from \url{http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip}\todo{reference}, and 100 images were assignet to training, 100 to development and the remaining 700 to test.

%dataset paper: http://www.vision.ee.ethz.ch/~timofter/publications/Agustsson-CVPRW-2017.pdf
% @InProceedings{Agustsson_2017_CVPR_Workshops,
% author = {Agustsson, Eirikur and Timofte, Radu},
% title = {NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study},
% booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
% month = {July},
% year = {2017}
% }

\section{Environment}
The experiments did not take advantage of GPU acceleration and were  conducted on a single computer with 256GB of RAM and with 2 Intel\textregistered Xeon\textregistered E5-2630 v2 processors, with 6 cores each, with 2 hyper-threads per core, or 24 hyper-threads in total. 

The source code for the experiments is available at \url{http://github.com/atilaromero/ML}.

\section{Results}

% \noindent
% \begin{algorithm}
% \begin{lstlisting}[language=Python, frame=single, numbers=left]
% last = l0 = tf.keras.layers.Input(shape=(512,256))
% last = Conv1D(16, (4,), padding="same", activation="relu")(last)
% last = MaxPooling1D(pool_size=(2,))(last)
% last = LSTM(16, return_sequences=False, dropout=0.5,
%             kernel_initializer=tf.keras.initializers.Ones())(last)
% last = Dense(3)(last)
% last = Activation('softmax')(last)

% model = tf.keras.Model([l0], last)
% \end{lstlisting}
% \caption{\label{alg:3-files-001}Experiment 3-files-001}
% \end{algorithm}


\subsection{First block as input}
In the first set of experiments, for each file selected as input sample from the test dataset, only the first 512 bytes of that file were used. For most filetypes, including those three under consideration, these first bytes contains regular patterns in fixed positions, which makes them easy to recognize.



Three network configurations were compared. The simplest one was a feedforward network, referred as ``D'', with only a single fully connected layer, with 131072 input units (512 bytes times 256 possible values using one hot encoding) and 3 output units, one for each output class, PDF, JPG, or PNG.

Another one, referred as ``LD'', used a LSTM layer with 32 output units, followed by a fully connected layer of 3 output units. The LSTM layer process one byte per time step. This is one of the simplest LSTM configurations. While it would be possible to use only a single LSTM layer without a fully connected layer next, then the LSTM layer would have to be limited to 3 output units, and would have too few trainable parameters. 

The last one, referred as ``CL'', used a convolutional layer without maxpooling with 32 input units, 3 output units, and a stride of 32 followed by a LSTM layer with 3 output units. Some other convolution sizes and strides were tried during the test development. A convolutional layer with 512 units of input would be equivalent to a fully connected layer, as no convolution would be performed. Using 32 input units and a stride of same size, the convolutional layer breaks the block in 16 parts with 32 bytes each. The convolutional layer already outputs the exact number of classes, thus the LSTM layer here is just summarizing the results.

\todo[inline]{check results again}
Table \ref{tab:carving7-11} gives a summary of the results of this set of experiments, listing for each model the number of trainable parameters, if the training used all of the blocks of the sample file or just the first one, the number of training epochs, the duration of the training, and the final accuracy on the training and on the validation datasets.
\input{content/5.3-z-tab-carving7-11.tex}

The simple feedforward network was by far the fastest networking concerning training time, and had the best validation accuracy. It is interesting to notice that the LSTM without a previous convolutional network had a slow training time.

Since the simplest network was giving the best results, the next experimentation step was to check if this performance would be maintained in a more complex scenario.

\subsection{Random block as input}

The next set of experiments uses a random block (512 bytes chunks) from each file, instead of just the first one. This is a harder classification task because, while in the first block is reasonable to find patterns in specific positions in relation to the beginning of the block, this correspondence is not normally preserved in the remaining blocks, as the pattern may start anywhere in the block. The second factor is that in files with low compression rates, as image files, the beginning of the file normally presents more recognizable patterns than the middle.

Eleven network configurations are here compared.
%18 CMCML, 20 CMCMLL
The best group of results are composed by two network configurations that use two convolutional layers with a max pooling followed by one or two LSTM layers, referred as ``CMCML'' and  ``CMCMLL''. The results for using one or two LSTM layers were very similar. These networks process 8 bytes
% , taking 33 and 30 minutes each, with 88.9\% and 88.6\% of accuracy in the validation dataset. 

%12 D
The simple feedforward network that performed well classifying the first block was unable to achieve similar results when classifying a random block.
% In 600 epochs, it only reached an accuracy of 77.5\% on the validation dataset, taking 62 minutes.

%LD
\todo[inline]{comment LD}

%%%% 13-CLL 23-CLD 16-CML 15-CL
The second best group of results was achieved by four networks with similar characteristics. One of then uses a convolutional layer followed by a LSTM layer, and the other three are variations of it, by adding a second LSTM layer, or by adding a fully connected layer at the end, or by adding maxpooling to the convolutional layer. These four networks presented similar results.

% 25-CD, 26-CM, 27-CCM
Three networks were constructed using convolutional layers without a LSTM layer. One of them uses a convolutional layer followed by a fully connected layer. Another one uses a convolutional layer followed by a maxpooling layer. And the third uses 2 convolutional layers and a maxpooling layer.


\todo[inline]{explain convolutional choices}
\todo[inline]{detail each experiment}

%%%%
Table \ref{tab:carving12-27} gives a summary of the results of this set of experiments, listing for each model the number of trainable parameters, if the training used all of the blocks of the sample file or just the first one, the number of training epochs, the duration of the training, and the final accuracy on the training and on the validation datasets.
\input{content/5.3-z-tab-carving12-27.tex}

Table \ref{tab:carvinglayers} specify the number of output units in each layer of the networks used in the experiments. For the networks with two convolutional layers, those layers are connected together before the LSTM layer. For the convolutional layers, the input number indicates the size of the receptive field. 
\input{content/5.3-z-tab-carving-layers.tex}

